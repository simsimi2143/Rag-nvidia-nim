{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l66f-4wDLYb"
      },
      "source": [
        "## Instalacion de librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa-jjozhDNu2",
        "outputId": "9722ea0f-9632-41d8-b35f-3c2df098c555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community faiss-cpu pypdf tiktoken langchain-nvidia-ai-endpoints -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbiH40x1Egdz"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementacion de rag con FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ohqHUHDUVI",
        "outputId": "261bab0c-2326-438e-e8ae-5aeb5e55cd98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/_common.py:217: UserWarning: Found nvidia/llama-3.3-nemotron-super-49b-v1 in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌟 Asistente RAG - Basado en tu PDF 📄\n",
            "\n",
            "\n",
            "🤔 Tu pregunta (o 'salir'): cual es el objetivo general \n",
            "\n",
            "💡 Respuesta:\n",
            "Basándome en el contexto proporcionado, puedo responder directamente a tu pregunta:\n",
            "\n",
            "**Objetivo General (1.3.1.)**\n",
            "\n",
            "* **Descripción**: Desarrollar una herramienta basada en inteligencia artificial que sintetice información a partir de links de YouTube y documentos en formato PDF, DOCX y PPT, permitiendo la generación de resúmenes automáticos y cuestionarios interactivos para el apoyo del aprendizaje.\n",
            "\n",
            "En resumen, el objetivo general es **desarrollar una herramienta de IA para sintetizar contenido de múltiples fuentes (YouTube, PDF, DOCX, PPT) y generar resúmenes y cuestionarios para apoyar el aprendizaje**.\n",
            "\n",
            "📚 Fuentes relevantes:\n",
            "1. Página 15: 1.3.1. Objetivo general Desarrollar una herramienta basada en inteligencia artificial que sintetice ...\n",
            "2. Página 5: los momentos difíciles. También quiero reconocer a mi familia por su amor incondicional y apoyo cons...\n",
            "\n",
            "🤔 Tu pregunta (o 'salir'): cual es la capital de egipto\n",
            "\n",
            "💡 Respuesta:\n",
            "Lo siento, pero después de revisar el contexto proporcionado, no encontré información relacionada con geografía, países, capitales o específicamente sobre Egipto. El texto se enfoca en técnicas de procesamiento de lenguaje natural (NLP), resumidos de textos, transcripción de videos, y comparativas de librerías y APIs para estos fines.\n",
            "\n",
            "**Respuesta:** No conozco la capital de Egipto con base en el contexto proporcionado. Sin embargo, si deseas la respuesta de manera general (fuera del contexto):\n",
            "\n",
            "**Respuesta General (No basada en el contexto):** La capital de Egipto es **El Cairo**.\n",
            "\n",
            "📚 Fuentes relevantes:\n",
            "1. Página 10: Para mejorar la coherencia del resumen, se incorpora una segmentación textual basada en fragmentos d...\n",
            "2. Página 22: modificar los parámetros de lenguaje (idioma latinoamericano específico en el cual obtener la transc...\n",
            "\n",
            "🤔 Tu pregunta (o 'salir'): que es el textrank\n",
            "\n",
            "💡 Respuesta:\n",
            "Con base en el contexto proporcionado, te puedo ofrecer una respuesta detallada sobre qué es TextRank:\n",
            "\n",
            "**¿Qué es TextRank?**\n",
            "\n",
            "TextRank es una técnica basada en grafos utilizada primariamente para la **extracción de resúmenes automáticos** de textos. También se puede aplicar en otras tareas como la identificación de palabras clave.\n",
            "\n",
            "**Funcionamiento Básico:**\n",
            "\n",
            "1. **Representación Gráfica**: El texto se representa como un grafo dirigido `G = (V, E)`, donde:\n",
            "   - **V** es el conjunto de **oraciones** (o a veces, palabras/palabras clave, dependiendo de la aplicación) del texto.\n",
            "   - **E** son las **conexiones entre estas oraciones** (o elementos), que tienen **pesos** asignados.\n",
            "\n",
            "2. **Asignación de Pesos**: El peso de cada conexión entre dos oraciones (o elementos) se calcula mediante la **similitud coseno** (`S(i, j) = ∑k WikWjkq`), que mide la similitud semántica o de contenido entre ellas.\n",
            "\n",
            "3. **Proceso Iterativo**: Se emplea un algoritmo **iterativo similar al PageRank** (famoso por su uso en Google para ranquear páginas web). Este proceso asigna un **score de relevancia** a cada oración (o elemento) basado en la estructura del grafo y los pesos de las conexiones. Las oraciones con mayor score son consideradas más relevantes para el resumen.\n",
            "\n",
            "**Características Clave mencionadas en el contexto:**\n",
            "\n",
            "- **Eficiencia**: Identifica oraciones relevantes de manera eficiente.\n",
            "- **Robustez y Escalabilidad**: Funciona bien con textos largos y grandes conjuntos de datos sin comprometer la precisión.\n",
            "- **Mejoras en Procesamiento**:\n",
            "  - **Reducción del Tiempo de Procesamiento**.\n",
            "  - **Código más Simple y Mantenible**.\n",
            "  - **Mejora en el Rendimiento con Textos Largos**.\n",
            "\n",
            "**Resumen:**\n",
            "TextRank es una técnica de procesamiento de lenguaje natural (NLP) que utiliza grafos para identificar y ranquear la relevancia de oraciones (o palabras) en un texto, ideal para resúmenes automáticos, destacando su eficiencia, robustez y capacidad para manejar grandes volúmenes de texto.\n",
            "\n",
            "📚 Fuentes relevantes:\n",
            "1. Página 18: CAPÍTULO 2. MARCO TEÓRICO 7 de voz. Una vez obtenida la transcripción, el texto es procesado y segme...\n",
            "2. Página 44: iterativo similar al utilizado en el algoritmo PageRank, lo que permite identificar las oraciones má...\n",
            "\n",
            "🤔 Tu pregunta (o 'salir'): salir\n",
            "\n",
            "¡Gracias por usar el asistente! 🎓\n"
          ]
        }
      ],
      "source": [
        "# 1. Configuración inicial\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from config.api_config import NVIDIA_API_KEY, PDF_PATH\n",
        "\n",
        "# 2. Cargar dependencias\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "import tiktoken\n",
        "\n",
        "# 3. Función para conteo de tokens\n",
        "def count_tokens(text: str) -> int:\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "# 4. Procesamiento de documentos\n",
        "def process_documents(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "    \n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=200,\n",
        "        chunk_overlap=30,\n",
        "        length_function=count_tokens,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "# 5. Creación de vectorstore\n",
        "def create_vectorstore(splits):\n",
        "    embedder = NVIDIAEmbeddings()\n",
        "    return FAISS.from_documents(documents=splits, embedding=embedder)\n",
        "\n",
        "# 6. Configuración de la cadena QA\n",
        "def setup_qa_chain(vectorstore):\n",
        "    llm = ChatNVIDIA(model=\"nvidia/llama-3.3-nemotron-super-49b-v1\")\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "# 7. Interfaz de usuario\n",
        "def run_chat(qa_chain):\n",
        "    def format_source(doc):\n",
        "        page = doc.metadata.get('page', 0) + 1\n",
        "        content = doc.page_content[:100].replace('\\n', ' ')\n",
        "        return f\"Página {page}: {content}...\"\n",
        "\n",
        "    print(\"🌟 Asistente RAG - Basado en tu PDF 📄\\n\")\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"\\n🤔 Tu pregunta (o 'salir'): \").strip()\n",
        "            if query.lower() in ['salir', 'exit', 'quit']:\n",
        "                break\n",
        "\n",
        "            result = qa_chain.invoke({\"query\": query})\n",
        "            \n",
        "            print(\"\\n💡 Respuesta:\")\n",
        "            print(result[\"result\"])\n",
        "\n",
        "            if result.get(\"source_documents\"):\n",
        "                print(\"\\n📚 Fuentes relevantes:\")\n",
        "                for i, doc in enumerate(result[\"source_documents\"][:2], 1):\n",
        "                    print(f\"{i}. {format_source(doc)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n⚠️ Error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "# 8. Ejecución principal\n",
        "if __name__ == \"__main__\":\n",
        "    # Cargar configuración\n",
        "    load_dotenv()\n",
        "    \n",
        "    # Pipeline completo\n",
        "    splits = process_documents(PDF_PATH)\n",
        "    vectorstore = create_vectorstore(splits)\n",
        "    qa_chain = setup_qa_chain(vectorstore)\n",
        "    \n",
        "    # Iniciar chat\n",
        "    run_chat(qa_chain)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
