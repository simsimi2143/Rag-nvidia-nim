{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l66f-4wDLYb"
      },
      "source": [
        "## Instalacion de librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa-jjozhDNu2",
        "outputId": "9722ea0f-9632-41d8-b35f-3c2df098c555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.4/303.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community faiss-cpu pypdf tiktoken langchain-nvidia-ai-endpoints -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbiH40x1Egdz"
      },
      "outputs": [],
      "source": [
        "!pip install tiktoken -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Implementacion de rag con FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4ohqHUHDUVI",
        "outputId": "261bab0c-2326-438e-e8ae-5aeb5e55cd98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_nvidia_ai_endpoints/_common.py:217: UserWarning: Found nvidia/llama-3.3-nemotron-super-49b-v1 in available_models, but type is unknown and inference may fail.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üåü Asistente RAG - Basado en tu PDF üìÑ\n",
            "\n",
            "\n",
            "ü§î Tu pregunta (o 'salir'): cual es el objetivo general \n",
            "\n",
            "üí° Respuesta:\n",
            "Bas√°ndome en el contexto proporcionado, puedo responder directamente a tu pregunta:\n",
            "\n",
            "**Objetivo General (1.3.1.)**\n",
            "\n",
            "* **Descripci√≥n**: Desarrollar una herramienta basada en inteligencia artificial que sintetice informaci√≥n a partir de links de YouTube y documentos en formato PDF, DOCX y PPT, permitiendo la generaci√≥n de res√∫menes autom√°ticos y cuestionarios interactivos para el apoyo del aprendizaje.\n",
            "\n",
            "En resumen, el objetivo general es **desarrollar una herramienta de IA para sintetizar contenido de m√∫ltiples fuentes (YouTube, PDF, DOCX, PPT) y generar res√∫menes y cuestionarios para apoyar el aprendizaje**.\n",
            "\n",
            "üìö Fuentes relevantes:\n",
            "1. P√°gina 15: 1.3.1. Objetivo general Desarrollar una herramienta basada en inteligencia artificial que sintetice ...\n",
            "2. P√°gina 5: los momentos dif√≠ciles. Tambi√©n quiero reconocer a mi familia por su amor incondicional y apoyo cons...\n",
            "\n",
            "ü§î Tu pregunta (o 'salir'): cual es la capital de egipto\n",
            "\n",
            "üí° Respuesta:\n",
            "Lo siento, pero despu√©s de revisar el contexto proporcionado, no encontr√© informaci√≥n relacionada con geograf√≠a, pa√≠ses, capitales o espec√≠ficamente sobre Egipto. El texto se enfoca en t√©cnicas de procesamiento de lenguaje natural (NLP), resumidos de textos, transcripci√≥n de videos, y comparativas de librer√≠as y APIs para estos fines.\n",
            "\n",
            "**Respuesta:** No conozco la capital de Egipto con base en el contexto proporcionado. Sin embargo, si deseas la respuesta de manera general (fuera del contexto):\n",
            "\n",
            "**Respuesta General (No basada en el contexto):** La capital de Egipto es **El Cairo**.\n",
            "\n",
            "üìö Fuentes relevantes:\n",
            "1. P√°gina 10: Para mejorar la coherencia del resumen, se incorpora una segmentaci√≥n textual basada en fragmentos d...\n",
            "2. P√°gina 22: modificar los par√°metros de lenguaje (idioma latinoamericano espec√≠fico en el cual obtener la transc...\n",
            "\n",
            "ü§î Tu pregunta (o 'salir'): que es el textrank\n",
            "\n",
            "üí° Respuesta:\n",
            "Con base en el contexto proporcionado, te puedo ofrecer una respuesta detallada sobre qu√© es TextRank:\n",
            "\n",
            "**¬øQu√© es TextRank?**\n",
            "\n",
            "TextRank es una t√©cnica basada en grafos utilizada primariamente para la **extracci√≥n de res√∫menes autom√°ticos** de textos. Tambi√©n se puede aplicar en otras tareas como la identificaci√≥n de palabras clave.\n",
            "\n",
            "**Funcionamiento B√°sico:**\n",
            "\n",
            "1. **Representaci√≥n Gr√°fica**: El texto se representa como un grafo dirigido `G = (V, E)`, donde:\n",
            "   - **V** es el conjunto de **oraciones** (o a veces, palabras/palabras clave, dependiendo de la aplicaci√≥n) del texto.\n",
            "   - **E** son las **conexiones entre estas oraciones** (o elementos), que tienen **pesos** asignados.\n",
            "\n",
            "2. **Asignaci√≥n de Pesos**: El peso de cada conexi√≥n entre dos oraciones (o elementos) se calcula mediante la **similitud coseno** (`S(i, j) = ‚àëk WikWjkq`), que mide la similitud sem√°ntica o de contenido entre ellas.\n",
            "\n",
            "3. **Proceso Iterativo**: Se emplea un algoritmo **iterativo similar al PageRank** (famoso por su uso en Google para ranquear p√°ginas web). Este proceso asigna un **score de relevancia** a cada oraci√≥n (o elemento) basado en la estructura del grafo y los pesos de las conexiones. Las oraciones con mayor score son consideradas m√°s relevantes para el resumen.\n",
            "\n",
            "**Caracter√≠sticas Clave mencionadas en el contexto:**\n",
            "\n",
            "- **Eficiencia**: Identifica oraciones relevantes de manera eficiente.\n",
            "- **Robustez y Escalabilidad**: Funciona bien con textos largos y grandes conjuntos de datos sin comprometer la precisi√≥n.\n",
            "- **Mejoras en Procesamiento**:\n",
            "  - **Reducci√≥n del Tiempo de Procesamiento**.\n",
            "  - **C√≥digo m√°s Simple y Mantenible**.\n",
            "  - **Mejora en el Rendimiento con Textos Largos**.\n",
            "\n",
            "**Resumen:**\n",
            "TextRank es una t√©cnica de procesamiento de lenguaje natural (NLP) que utiliza grafos para identificar y ranquear la relevancia de oraciones (o palabras) en un texto, ideal para res√∫menes autom√°ticos, destacando su eficiencia, robustez y capacidad para manejar grandes vol√∫menes de texto.\n",
            "\n",
            "üìö Fuentes relevantes:\n",
            "1. P√°gina 18: CAP√çTULO 2. MARCO TE√ìRICO 7 de voz. Una vez obtenida la transcripci√≥n, el texto es procesado y segme...\n",
            "2. P√°gina 44: iterativo similar al utilizado en el algoritmo PageRank, lo que permite identificar las oraciones m√°...\n",
            "\n",
            "ü§î Tu pregunta (o 'salir'): salir\n",
            "\n",
            "¬°Gracias por usar el asistente! üéì\n"
          ]
        }
      ],
      "source": [
        "# 1. Configuraci√≥n inicial\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from config.api_config import NVIDIA_API_KEY, PDF_PATH\n",
        "\n",
        "# 2. Cargar dependencias\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "import tiktoken\n",
        "\n",
        "# 3. Funci√≥n para conteo de tokens\n",
        "def count_tokens(text: str) -> int:\n",
        "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    return len(tokenizer.encode(text))\n",
        "\n",
        "# 4. Procesamiento de documentos\n",
        "def process_documents(pdf_path):\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    documents = loader.load()\n",
        "    \n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=200,\n",
        "        chunk_overlap=30,\n",
        "        length_function=count_tokens,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \" \", \"\"]\n",
        "    )\n",
        "    \n",
        "    return text_splitter.split_documents(documents)\n",
        "\n",
        "# 5. Creaci√≥n de vectorstore\n",
        "def create_vectorstore(splits):\n",
        "    embedder = NVIDIAEmbeddings()\n",
        "    return FAISS.from_documents(documents=splits, embedding=embedder)\n",
        "\n",
        "# 6. Configuraci√≥n de la cadena QA\n",
        "def setup_qa_chain(vectorstore):\n",
        "    llm = ChatNVIDIA(model=\"nvidia/llama-3.3-nemotron-super-49b-v1\")\n",
        "    return RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 2}),\n",
        "        return_source_documents=True\n",
        "    )\n",
        "\n",
        "# 7. Interfaz de usuario\n",
        "def run_chat(qa_chain):\n",
        "    def format_source(doc):\n",
        "        page = doc.metadata.get('page', 0) + 1\n",
        "        content = doc.page_content[:100].replace('\\n', ' ')\n",
        "        return f\"P√°gina {page}: {content}...\"\n",
        "\n",
        "    print(\"üåü Asistente RAG - Basado en tu PDF üìÑ\\n\")\n",
        "    while True:\n",
        "        try:\n",
        "            query = input(\"\\nü§î Tu pregunta (o 'salir'): \").strip()\n",
        "            if query.lower() in ['salir', 'exit', 'quit']:\n",
        "                break\n",
        "\n",
        "            result = qa_chain.invoke({\"query\": query})\n",
        "            \n",
        "            print(\"\\nüí° Respuesta:\")\n",
        "            print(result[\"result\"])\n",
        "\n",
        "            if result.get(\"source_documents\"):\n",
        "                print(\"\\nüìö Fuentes relevantes:\")\n",
        "                for i, doc in enumerate(result[\"source_documents\"][:2], 1):\n",
        "                    print(f\"{i}. {format_source(doc)}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n‚ö†Ô∏è Error: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "# 8. Ejecuci√≥n principal\n",
        "if __name__ == \"__main__\":\n",
        "    # Cargar configuraci√≥n\n",
        "    load_dotenv()\n",
        "    \n",
        "    # Pipeline completo\n",
        "    splits = process_documents(PDF_PATH)\n",
        "    vectorstore = create_vectorstore(splits)\n",
        "    qa_chain = setup_qa_chain(vectorstore)\n",
        "    \n",
        "    # Iniciar chat\n",
        "    run_chat(qa_chain)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
